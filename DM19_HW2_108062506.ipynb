{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: é™³ç¾¿å…ˆ\n",
    "\n",
    "Student ID: 108062506\n",
    "\n",
    "GitHub ID: s38565\n",
    "\n",
    "Kaggle name: sh1llyixian\n",
    "\n",
    "Kaggle private scoreboard snapshot:\n",
    "\n",
    "[Snapshot](img/pic0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home** exercises in the [DM19-Lab2-Master Repo](https://github.com/EvaArevalo/DM19-Lab2-Master). You may need to copy some cells from the Lab notebook to this notebook. \n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/t/179d01d4dd984fc5ac45a894822479dd) regarding Emotion Recognition on Twitter. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20% of the score (ie. 20% of 30% )\n",
    "\n",
    "    - **Top 41% - 100%**: Get (101-x)% of the score, where x is your ranking in the leaderboard (ie. (101-x)% of 30% )   \n",
    "    Submit your last submission __BEFORE the deadline (Nov. 23rd 11:59 pm, Saturday)__. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developping the model for the competition (You can use code and comment it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook** and **add minimal comments where needed**.\n",
    "\n",
    "\n",
    "You can submit your homework following these guidelines: [Git Intro & How to hand your homework](https://github.com/EvaArevalo/DM19-Lab1-Master/blob/master/Git%20Intro%20%26%20How%20to%20hand%20your%20homework.ipynb), but make sure to fork the [DM19-Lab2-Homework](https://github.com/EvaArevalo/DM19-Lab2-Homework) repository this time! Also please __DONÂ´T UPLOAD HUGE DOCUMENTS__, please use Git ignore for that.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Nov. 26th 11:59 pm, Tuesday)__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T06:32:43.236089Z",
     "start_time": "2019-11-28T06:32:42.780687Z"
    }
   },
   "outputs": [],
   "source": [
    "# import related library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import re, csv\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, I first try deep learning NN model as lab showed, but I finally found that it's not a good model due to my bad prediction result.<br>\n",
    "So the first part of the code is to show my trying process.<br>\n",
    "And the second part is the LSTM model I used in compitition.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading related raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, I load all data in and merge them into single dataframe format and finally seperate train/test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T07:40:25.448407Z",
     "start_time": "2019-11-26T07:40:19.285349Z"
    }
   },
   "outputs": [],
   "source": [
    "# load csv and restructure dataframe (rename)\n",
    "iden_df = pd.read_csv(\"data_identification.csv\", sep=\"\\t\")\n",
    "iden_df['tweet_id'] = iden_df['tweet_id,identification'].str.split(',', expand=True)[0]\n",
    "iden_df['identification'] = iden_df['tweet_id,identification'].str.split(',', expand=True)[1]\n",
    "iden_df = iden_df.drop('tweet_id,identification', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T03:31:28.224780Z",
     "start_time": "2019-11-25T03:31:28.211621Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>identification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x28cc61</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x29e452</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x2b3819</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2db41f</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2a2acc</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id identification\n",
       "0  0x28cc61           test\n",
       "1  0x29e452          train\n",
       "2  0x2b3819          train\n",
       "3  0x2db41f           test\n",
       "4  0x2a2acc          train"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iden_df.head() # show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T03:31:30.379411Z",
     "start_time": "2019-11-25T03:31:30.370580Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1867535, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see shape of dataframe\n",
    "iden_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T03:31:38.006581Z",
     "start_time": "2019-11-25T03:31:31.832692Z"
    }
   },
   "outputs": [],
   "source": [
    "# load csv and restructure dataframe (rename)\n",
    "emotion_df = pd.read_csv(\"emotion.csv\", sep=\"\\t\")\n",
    "emotion_df['tweet_id'] = emotion_df['tweet_id,emotion'].str.split(',', expand=True)[0]\n",
    "emotion_df['emotion'] = emotion_df['tweet_id,emotion'].str.split(',', expand=True)[1]\n",
    "emotion_df = emotion_df.drop('tweet_id,emotion', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T03:31:38.012337Z",
     "start_time": "2019-11-25T03:31:38.008033Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x3140b1</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x368b73</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x296183</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2bd6e1</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2ee1dd</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id       emotion\n",
       "0  0x3140b1       sadness\n",
       "1  0x368b73       disgust\n",
       "2  0x296183  anticipation\n",
       "3  0x2bd6e1           joy\n",
       "4  0x2ee1dd  anticipation"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_df.head() # show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T03:31:41.131201Z",
     "start_time": "2019-11-25T03:31:41.122638Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1455563, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see shape of dataframe\n",
    "emotion_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T03:56:17.580792Z",
     "start_time": "2019-11-25T03:56:02.785530Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_score</th>\n",
       "      <th>_index</th>\n",
       "      <th>_source</th>\n",
       "      <th>_crawldate</th>\n",
       "      <th>_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>391</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['Snapchat'], 'tweet_id...</td>\n",
       "      <td>2015-05-23 11:42:47</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>433</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['freepress', 'TrumpLeg...</td>\n",
       "      <td>2016-01-28 04:52:09</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>232</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': ['bibleverse'], 'tweet_...</td>\n",
       "      <td>2017-12-25 04:39:20</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>376</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': [], 'tweet_id': '0x1cd5...</td>\n",
       "      <td>2016-01-24 23:53:05</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>989</td>\n",
       "      <td>hashtag_tweets</td>\n",
       "      <td>{'tweet': {'hashtags': [], 'tweet_id': '0x2de2...</td>\n",
       "      <td>2016-01-08 17:18:59</td>\n",
       "      <td>tweets</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _score          _index                                            _source  \\\n",
       "0     391  hashtag_tweets  {'tweet': {'hashtags': ['Snapchat'], 'tweet_id...   \n",
       "1     433  hashtag_tweets  {'tweet': {'hashtags': ['freepress', 'TrumpLeg...   \n",
       "2     232  hashtag_tweets  {'tweet': {'hashtags': ['bibleverse'], 'tweet_...   \n",
       "3     376  hashtag_tweets  {'tweet': {'hashtags': [], 'tweet_id': '0x1cd5...   \n",
       "4     989  hashtag_tweets  {'tweet': {'hashtags': [], 'tweet_id': '0x2de2...   \n",
       "\n",
       "            _crawldate   _type  \n",
       "0  2015-05-23 11:42:47  tweets  \n",
       "1  2016-01-28 04:52:09  tweets  \n",
       "2  2017-12-25 04:39:20  tweets  \n",
       "3  2016-01-24 23:53:05  tweets  \n",
       "4  2016-01-08 17:18:59  tweets  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load raw data of twitter\n",
    "datas = open('tweets_DM.json').read()\n",
    "tweet_DM = pd.read_json(datas, lines = True)\n",
    "tweet_DM.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T03:56:58.361475Z",
     "start_time": "2019-11-25T03:56:29.901214Z"
    }
   },
   "outputs": [],
   "source": [
    "# get some useful information from all data\n",
    "train_df = json_normalize(tweet_DM['_source'])\n",
    "train_df['tweet_id'] = train_df['tweet.tweet_id']\n",
    "train_df = train_df.drop('tweet.tweet_id', axis=1)\n",
    "train_df['text'] = train_df['tweet.text']\n",
    "train_df = train_df.drop('tweet.text', axis=1)\n",
    "train_df['hashtags'] = train_df['tweet.hashtags']\n",
    "train_df = train_df.drop('tweet.hashtags', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T03:57:02.633723Z",
     "start_time": "2019-11-25T03:57:02.625384Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>[Snapchat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>[bibleverse]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                               text  \\\n",
       "0  0x376b20  People who post \"add me on #Snapchat\" must be ...   \n",
       "1  0x2d5350  @brianklaas As we see, Trump is dangerous to #...   \n",
       "2  0x28b412  Confident of your obedience, I write to you, k...   \n",
       "3  0x1cd5b0                Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>   \n",
       "4  0x2de201  \"Trust is not the same as faith. A friend is s...   \n",
       "\n",
       "                        hashtags  \n",
       "0                     [Snapchat]  \n",
       "1  [freepress, TrumpLegacy, CNN]  \n",
       "2                   [bibleverse]  \n",
       "3                             []  \n",
       "4                             []  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head() # show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T03:57:07.294345Z",
     "start_time": "2019-11-25T03:57:07.285404Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1867535, 3)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see shape of dataframe\n",
    "train_df.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T03:57:26.193552Z",
     "start_time": "2019-11-25T03:57:25.125120Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          False\n",
       "1          False\n",
       "2          False\n",
       "3          False\n",
       "4          False\n",
       "           ...  \n",
       "1867530    False\n",
       "1867531    False\n",
       "1867532    False\n",
       "1867533    False\n",
       "1867534    False\n",
       "Length: 1867535, dtype: bool"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if duplicated\n",
    "emotion_df.duplicated(keep='first')\n",
    "iden_df.duplicated(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T03:57:40.738971Z",
     "start_time": "2019-11-25T03:57:38.950351Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0x368e95</td>\n",
       "      <td>Love knows no gender. ðŸ˜¢ðŸ˜­ &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0x249c0c</td>\n",
       "      <td>@DStvNgCare @DStvNg More highlights are being ...</td>\n",
       "      <td>[LeagueCup]</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0x218443</td>\n",
       "      <td>When do you have enough ? When are you satisfi...</td>\n",
       "      <td>[materialism, money, possessions]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                               text  \\\n",
       "0  0x376b20  People who post \"add me on #Snapchat\" must be ...   \n",
       "1  0x2d5350  @brianklaas As we see, Trump is dangerous to #...   \n",
       "2  0x28b412  Confident of your obedience, I write to you, k...   \n",
       "3  0x1cd5b0                Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>   \n",
       "4  0x2de201  \"Trust is not the same as faith. A friend is s...   \n",
       "5  0x1d755c  @RISKshow @TheKevinAllison Thx for the BEST TI...   \n",
       "6  0x2c91a8       Still waiting on those supplies Liscus. <LH>   \n",
       "7  0x368e95                      Love knows no gender. ðŸ˜¢ðŸ˜­ <LH>   \n",
       "8  0x249c0c  @DStvNgCare @DStvNg More highlights are being ...   \n",
       "9  0x218443  When do you have enough ? When are you satisfi...   \n",
       "\n",
       "                            hashtags       emotion  \n",
       "0                         [Snapchat]  anticipation  \n",
       "1      [freepress, TrumpLegacy, CNN]       sadness  \n",
       "2                       [bibleverse]           NaN  \n",
       "3                                 []          fear  \n",
       "4                                 []           NaN  \n",
       "5          [authentic, LaughOutLoud]           joy  \n",
       "6                                 []  anticipation  \n",
       "7                                 []           joy  \n",
       "8                        [LeagueCup]       sadness  \n",
       "9  [materialism, money, possessions]           NaN  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge data sets\n",
    "train_df_merge = pd.merge(train_df, emotion_df, how='outer', on=['tweet_id'])\n",
    "train_df_merge.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T03:57:47.293651Z",
     "start_time": "2019-11-25T03:57:45.415202Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>emotion</th>\n",
       "      <th>identification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>sadness</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>fear</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0x368e95</td>\n",
       "      <td>Love knows no gender. ðŸ˜¢ðŸ˜­ &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0x249c0c</td>\n",
       "      <td>@DStvNgCare @DStvNg More highlights are being ...</td>\n",
       "      <td>[LeagueCup]</td>\n",
       "      <td>sadness</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0x218443</td>\n",
       "      <td>When do you have enough ? When are you satisfi...</td>\n",
       "      <td>[materialism, money, possessions]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                               text  \\\n",
       "0  0x376b20  People who post \"add me on #Snapchat\" must be ...   \n",
       "1  0x2d5350  @brianklaas As we see, Trump is dangerous to #...   \n",
       "2  0x28b412  Confident of your obedience, I write to you, k...   \n",
       "3  0x1cd5b0                Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>   \n",
       "4  0x2de201  \"Trust is not the same as faith. A friend is s...   \n",
       "5  0x1d755c  @RISKshow @TheKevinAllison Thx for the BEST TI...   \n",
       "6  0x2c91a8       Still waiting on those supplies Liscus. <LH>   \n",
       "7  0x368e95                      Love knows no gender. ðŸ˜¢ðŸ˜­ <LH>   \n",
       "8  0x249c0c  @DStvNgCare @DStvNg More highlights are being ...   \n",
       "9  0x218443  When do you have enough ? When are you satisfi...   \n",
       "\n",
       "                            hashtags       emotion identification  \n",
       "0                         [Snapchat]  anticipation          train  \n",
       "1      [freepress, TrumpLegacy, CNN]       sadness          train  \n",
       "2                       [bibleverse]           NaN           test  \n",
       "3                                 []          fear          train  \n",
       "4                                 []           NaN           test  \n",
       "5          [authentic, LaughOutLoud]           joy          train  \n",
       "6                                 []  anticipation          train  \n",
       "7                                 []           joy          train  \n",
       "8                        [LeagueCup]       sadness          train  \n",
       "9  [materialism, money, possessions]           NaN           test  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge data sets\n",
    "train_df_merge = pd.merge(train_df_merge, iden_df, how='outer', on=['tweet_id'])\n",
    "train_df_merge.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T03:57:53.304879Z",
     "start_time": "2019-11-25T03:57:52.260514Z"
    }
   },
   "outputs": [],
   "source": [
    "# seperate training and testing data\n",
    "trains, tests = [x for _, x in train_df_merge.groupby(train_df_merge['identification'] == 'test')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T04:11:31.663829Z",
     "start_time": "2019-11-25T04:11:31.654860Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1455563, 5)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see shape of dataframe\n",
    "trains.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T04:36:41.039870Z",
     "start_time": "2019-11-25T04:36:41.030739Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(411972, 5)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see shape of dataframe\n",
    "tests.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T05:20:19.872488Z",
     "start_time": "2019-11-25T05:20:19.863151Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>emotion</th>\n",
       "      <th>identification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x28b412</td>\n",
       "      <td>Confident of your obedience, I write to you, k...</td>\n",
       "      <td>[bibleverse]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2de201</td>\n",
       "      <td>\"Trust is not the same as faith. A friend is s...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0x218443</td>\n",
       "      <td>When do you have enough ? When are you satisfi...</td>\n",
       "      <td>[materialism, money, possessions]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0x2939d5</td>\n",
       "      <td>God woke you up, now chase the day #GodsPlan #...</td>\n",
       "      <td>[GodsPlan, GodsWork]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0x26289a</td>\n",
       "      <td>In these tough times, who do YOU turn to as yo...</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    tweet_id                                               text  \\\n",
       "2   0x28b412  Confident of your obedience, I write to you, k...   \n",
       "4   0x2de201  \"Trust is not the same as faith. A friend is s...   \n",
       "9   0x218443  When do you have enough ? When are you satisfi...   \n",
       "30  0x2939d5  God woke you up, now chase the day #GodsPlan #...   \n",
       "33  0x26289a  In these tough times, who do YOU turn to as yo...   \n",
       "\n",
       "                             hashtags emotion identification  \n",
       "2                        [bibleverse]     NaN           test  \n",
       "4                                  []     NaN           test  \n",
       "9   [materialism, money, possessions]     NaN           test  \n",
       "30               [GodsPlan, GodsWork]     NaN           test  \n",
       "33                                 []     NaN           test  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tests.head() #show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T03:57:59.133233Z",
     "start_time": "2019-11-25T03:57:56.425525Z"
    }
   },
   "outputs": [],
   "source": [
    "## save to pickle file\n",
    "trains.to_pickle(\"train_df.pkl\") \n",
    "tests.to_pickle(\"test_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T05:55:11.372000Z",
     "start_time": "2019-11-27T05:55:09.134916Z"
    }
   },
   "outputs": [],
   "source": [
    "## load a pickle file\n",
    "trains = pd.read_pickle(\"train_df.pkl\")\n",
    "tests = pd.read_pickle(\"test_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T05:42:53.798250Z",
     "start_time": "2019-11-27T05:42:53.483536Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emotion\n",
       "anger            39867\n",
       "anticipation    248935\n",
       "disgust         139101\n",
       "fear             63999\n",
       "joy             516017\n",
       "sadness         193437\n",
       "surprise         48729\n",
       "trust           205478\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#group to find distribution\n",
    "trains.groupby(['emotion']).count()['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Related Work by: https://medium.com/@SeoJaeDuk/basic-data-cleaning-engineering-session-twitter-sentiment-data-b9376a91109b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1). remove stopwords from nltk library (cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first step of my preprocessing, I use nltk english stopwords library to clean my data.<br>\n",
    "This step can remove all not related vocabulary(not such meaningful for us) such as I,we, then......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T05:55:38.094397Z",
     "start_time": "2019-11-27T05:55:13.818017Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "stop = stopwords.words('english')\n",
    "trains['stop_words'] = trains['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T11:38:38.142123Z",
     "start_time": "2019-11-25T11:38:38.129723Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>emotion</th>\n",
       "      <th>identification</th>\n",
       "      <th>stop_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>People post \"add #Snapchat\" must dehydrated. C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>sadness</td>\n",
       "      <td>train</td>\n",
       "      <td>@brianklaas As see, Trump dangerous #freepress...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>fear</td>\n",
       "      <td>train</td>\n",
       "      <td>Now ISSA stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;LH&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx BEST TIME tonig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>Still waiting supplies Liscus. &lt;LH&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                               text  \\\n",
       "0  0x376b20  People who post \"add me on #Snapchat\" must be ...   \n",
       "1  0x2d5350  @brianklaas As we see, Trump is dangerous to #...   \n",
       "3  0x1cd5b0                Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>   \n",
       "5  0x1d755c  @RISKshow @TheKevinAllison Thx for the BEST TI...   \n",
       "6  0x2c91a8       Still waiting on those supplies Liscus. <LH>   \n",
       "\n",
       "                        hashtags       emotion identification  \\\n",
       "0                     [Snapchat]  anticipation          train   \n",
       "1  [freepress, TrumpLegacy, CNN]       sadness          train   \n",
       "3                             []          fear          train   \n",
       "5      [authentic, LaughOutLoud]           joy          train   \n",
       "6                             []  anticipation          train   \n",
       "\n",
       "                                          stop_words  \n",
       "0  People post \"add #Snapchat\" must dehydrated. C...  \n",
       "1  @brianklaas As see, Trump dangerous #freepress...  \n",
       "3                   Now ISSA stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>  \n",
       "5  @RISKshow @TheKevinAllison Thx BEST TIME tonig...  \n",
       "6                Still waiting supplies Liscus. <LH>  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trains.head() # show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2). Replace abbreviations and some spell correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In twitter, there are sometimes we'll talk colloquially.<br>\n",
    "So, we need to use some translator to change these kinds of abbreviation back.<br>\n",
    "And this can be done by slang.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T05:55:42.770643Z",
     "start_time": "2019-11-27T05:55:42.764779Z"
    }
   },
   "outputs": [],
   "source": [
    "# function to translate abbreviations\n",
    "def translator(user_string):\n",
    "    user_string = user_string.split(\" \")\n",
    "    j = 0\n",
    "    for _str in user_string:\n",
    "        # File path which consists of Abbreviations.\n",
    "        fileName = \"slang.txt\"\n",
    "        # File Access mode [Read Mode]\n",
    "        accessMode = \"r\"\n",
    "        with open(fileName, accessMode) as myCSVfile:\n",
    "            # Reading file as CSV with delimiter as \"=\", so that abbreviation are stored in row[0] and phrases in row[1]\n",
    "            dataFromFile = csv.reader(myCSVfile, delimiter=\"=\")\n",
    "            # Removing Special Characters.\n",
    "            _str = re.sub('[^a-zA-Z0-9-_.]', '', _str)\n",
    "            for row in dataFromFile:\n",
    "                # Check if selected word matches short forms[LHS] in text file.\n",
    "                if _str.upper() == row[0]:\n",
    "                    # If match found replace it with its appropriate phrase in text file.\n",
    "                    user_string[j] = row[1]\n",
    "        j = j + 1\n",
    "    # Replacing commas with spaces for final output.\n",
    "    return (' '.join(user_string))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T06:04:06.834342Z",
     "start_time": "2019-11-27T05:55:45.245364Z"
    }
   },
   "outputs": [],
   "source": [
    "# add columns for new step of preprocessing\n",
    "trains['sentimentText'] = trains['stop_words'].apply(lambda x: translator(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T12:04:13.583172Z",
     "start_time": "2019-11-25T12:04:13.573579Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>emotion</th>\n",
       "      <th>identification</th>\n",
       "      <th>stop_words</th>\n",
       "      <th>sentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>People post \"add #Snapchat\" must dehydrated. C...</td>\n",
       "      <td>People post \"add #Snapchat\" must dehydrated. C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>sadness</td>\n",
       "      <td>train</td>\n",
       "      <td>@brianklaas As see, Trump dangerous #freepress...</td>\n",
       "      <td>@brianklaas As see, Trump dangerous #freepress...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>fear</td>\n",
       "      <td>train</td>\n",
       "      <td>Now ISSA stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>Now ISSA stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;LH&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx BEST TIME tonig...</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thank You BEST TIME...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>Still waiting supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>Still waiting supplies Liscus. &lt;LH&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                               text  \\\n",
       "0  0x376b20  People who post \"add me on #Snapchat\" must be ...   \n",
       "1  0x2d5350  @brianklaas As we see, Trump is dangerous to #...   \n",
       "3  0x1cd5b0                Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>   \n",
       "5  0x1d755c  @RISKshow @TheKevinAllison Thx for the BEST TI...   \n",
       "6  0x2c91a8       Still waiting on those supplies Liscus. <LH>   \n",
       "\n",
       "                        hashtags       emotion identification  \\\n",
       "0                     [Snapchat]  anticipation          train   \n",
       "1  [freepress, TrumpLegacy, CNN]       sadness          train   \n",
       "3                             []          fear          train   \n",
       "5      [authentic, LaughOutLoud]           joy          train   \n",
       "6                             []  anticipation          train   \n",
       "\n",
       "                                          stop_words  \\\n",
       "0  People post \"add #Snapchat\" must dehydrated. C...   \n",
       "1  @brianklaas As see, Trump dangerous #freepress...   \n",
       "3                   Now ISSA stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>   \n",
       "5  @RISKshow @TheKevinAllison Thx BEST TIME tonig...   \n",
       "6                Still waiting supplies Liscus. <LH>   \n",
       "\n",
       "                                       sentimentText  \n",
       "0  People post \"add #Snapchat\" must dehydrated. C...  \n",
       "1  @brianklaas As see, Trump dangerous #freepress...  \n",
       "3                   Now ISSA stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>  \n",
       "5  @RISKshow @TheKevinAllison Thank You BEST TIME...  \n",
       "6                Still waiting supplies Liscus. <LH>  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trains.head() # show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3). Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we known, stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form.<br>\n",
    "And this step can prevent us from treating different part-of-speech  as different features and misidentify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T06:07:04.424843Z",
     "start_time": "2019-11-27T06:04:36.072641Z"
    }
   },
   "outputs": [],
   "source": [
    "# step 3 Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "trains['stemming'] = trains['sentimentText'].apply(lambda x: ' '.join([ps.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T12:07:11.576502Z",
     "start_time": "2019-11-25T12:07:11.566399Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>emotion</th>\n",
       "      <th>identification</th>\n",
       "      <th>stop_words</th>\n",
       "      <th>sentimentText</th>\n",
       "      <th>stemming</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>People post \"add #Snapchat\" must dehydrated. C...</td>\n",
       "      <td>People post \"add #Snapchat\" must dehydrated. C...</td>\n",
       "      <td>peopl post \"add #snapchat\" must dehydrated. cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>sadness</td>\n",
       "      <td>train</td>\n",
       "      <td>@brianklaas As see, Trump dangerous #freepress...</td>\n",
       "      <td>@brianklaas As see, Trump dangerous #freepress...</td>\n",
       "      <td>@brianklaa As see, trump danger #freepress aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>fear</td>\n",
       "      <td>train</td>\n",
       "      <td>Now ISSA stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>Now ISSA stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>now issa stalk tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;lh&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx BEST TIME tonig...</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thank You BEST TIME...</td>\n",
       "      <td>@riskshow @thekevinallison thank you best time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>Still waiting supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>Still waiting supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>still wait suppli liscus. &lt;lh&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                               text  \\\n",
       "0  0x376b20  People who post \"add me on #Snapchat\" must be ...   \n",
       "1  0x2d5350  @brianklaas As we see, Trump is dangerous to #...   \n",
       "3  0x1cd5b0                Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>   \n",
       "5  0x1d755c  @RISKshow @TheKevinAllison Thx for the BEST TI...   \n",
       "6  0x2c91a8       Still waiting on those supplies Liscus. <LH>   \n",
       "\n",
       "                        hashtags       emotion identification  \\\n",
       "0                     [Snapchat]  anticipation          train   \n",
       "1  [freepress, TrumpLegacy, CNN]       sadness          train   \n",
       "3                             []          fear          train   \n",
       "5      [authentic, LaughOutLoud]           joy          train   \n",
       "6                             []  anticipation          train   \n",
       "\n",
       "                                          stop_words  \\\n",
       "0  People post \"add #Snapchat\" must dehydrated. C...   \n",
       "1  @brianklaas As see, Trump dangerous #freepress...   \n",
       "3                   Now ISSA stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>   \n",
       "5  @RISKshow @TheKevinAllison Thx BEST TIME tonig...   \n",
       "6                Still waiting supplies Liscus. <LH>   \n",
       "\n",
       "                                       sentimentText  \\\n",
       "0  People post \"add #Snapchat\" must dehydrated. C...   \n",
       "1  @brianklaas As see, Trump dangerous #freepress...   \n",
       "3                   Now ISSA stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>   \n",
       "5  @RISKshow @TheKevinAllison Thank You BEST TIME...   \n",
       "6                Still waiting supplies Liscus. <LH>   \n",
       "\n",
       "                                            stemming  \n",
       "0  peopl post \"add #snapchat\" must dehydrated. cu...  \n",
       "1  @brianklaa As see, trump danger #freepress aro...  \n",
       "3                      now issa stalk tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <lh>  \n",
       "5  @riskshow @thekevinallison thank you best time...  \n",
       "6                     still wait suppli liscus. <lh>  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trains.head() #show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4). Lemmatize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the wordâ€™s lemma, or dictionary form.<br>\n",
    "And this process is very similar to stemming.<br>\n",
    "However one difference we can note is the fact that the word already have not changed to alreadi. <br>\n",
    "So I finally just keep this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T06:07:32.521671Z",
     "start_time": "2019-11-27T06:07:04.426126Z"
    }
   },
   "outputs": [],
   "source": [
    "# step 4 Lemmatize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lmtzr = WordNetLemmatizer()\n",
    "trains['lemmazation'] = trains['stemming'].apply(lambda x: ' '.join([lmtzr.lemmatize(word,'v') for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-25T12:07:57.300760Z",
     "start_time": "2019-11-25T12:07:57.225808Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>emotion</th>\n",
       "      <th>identification</th>\n",
       "      <th>stop_words</th>\n",
       "      <th>sentimentText</th>\n",
       "      <th>stemming</th>\n",
       "      <th>lemmazation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>People post \"add #Snapchat\" must dehydrated. C...</td>\n",
       "      <td>People post \"add #Snapchat\" must dehydrated. C...</td>\n",
       "      <td>peopl post \"add #snapchat\" must dehydrated. cu...</td>\n",
       "      <td>peopl post \"add #snapchat\" must dehydrated. cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>sadness</td>\n",
       "      <td>train</td>\n",
       "      <td>@brianklaas As see, Trump dangerous #freepress...</td>\n",
       "      <td>@brianklaas As see, Trump dangerous #freepress...</td>\n",
       "      <td>@brianklaa As see, trump danger #freepress aro...</td>\n",
       "      <td>@brianklaa As see, trump danger #freepress aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>fear</td>\n",
       "      <td>train</td>\n",
       "      <td>Now ISSA stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>Now ISSA stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>now issa stalk tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;lh&gt;</td>\n",
       "      <td>now issa stalk tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;lh&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx BEST TIME tonig...</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thank You BEST TIME...</td>\n",
       "      <td>@riskshow @thekevinallison thank you best time...</td>\n",
       "      <td>@riskshow @thekevinallison thank you best time...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>Still waiting supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>Still waiting supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>still wait suppli liscus. &lt;lh&gt;</td>\n",
       "      <td>still wait suppli liscus. &lt;lh&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                               text  \\\n",
       "0  0x376b20  People who post \"add me on #Snapchat\" must be ...   \n",
       "1  0x2d5350  @brianklaas As we see, Trump is dangerous to #...   \n",
       "3  0x1cd5b0                Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>   \n",
       "5  0x1d755c  @RISKshow @TheKevinAllison Thx for the BEST TI...   \n",
       "6  0x2c91a8       Still waiting on those supplies Liscus. <LH>   \n",
       "\n",
       "                        hashtags       emotion identification  \\\n",
       "0                     [Snapchat]  anticipation          train   \n",
       "1  [freepress, TrumpLegacy, CNN]       sadness          train   \n",
       "3                             []          fear          train   \n",
       "5      [authentic, LaughOutLoud]           joy          train   \n",
       "6                             []  anticipation          train   \n",
       "\n",
       "                                          stop_words  \\\n",
       "0  People post \"add #Snapchat\" must dehydrated. C...   \n",
       "1  @brianklaas As see, Trump dangerous #freepress...   \n",
       "3                   Now ISSA stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>   \n",
       "5  @RISKshow @TheKevinAllison Thx BEST TIME tonig...   \n",
       "6                Still waiting supplies Liscus. <LH>   \n",
       "\n",
       "                                       sentimentText  \\\n",
       "0  People post \"add #Snapchat\" must dehydrated. C...   \n",
       "1  @brianklaas As see, Trump dangerous #freepress...   \n",
       "3                   Now ISSA stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>   \n",
       "5  @RISKshow @TheKevinAllison Thank You BEST TIME...   \n",
       "6                Still waiting supplies Liscus. <LH>   \n",
       "\n",
       "                                            stemming  \\\n",
       "0  peopl post \"add #snapchat\" must dehydrated. cu...   \n",
       "1  @brianklaa As see, trump danger #freepress aro...   \n",
       "3                      now issa stalk tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <lh>   \n",
       "5  @riskshow @thekevinallison thank you best time...   \n",
       "6                     still wait suppli liscus. <lh>   \n",
       "\n",
       "                                         lemmazation  \n",
       "0  peopl post \"add #snapchat\" must dehydrated. cu...  \n",
       "1  @brianklaa As see, trump danger #freepress aro...  \n",
       "3                      now issa stalk tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <lh>  \n",
       "5  @riskshow @thekevinallison thank you best time...  \n",
       "6                     still wait suppli liscus. <lh>  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trains.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5).  Remove punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After above preprocessing process, I found that punctuation will be some noise information in training.<br>\n",
    "In addition, Twitter contains many #hashtag in text data. <br>\n",
    "If we just remove hashtag word, I think this will lose many useful information. <br>\n",
    "So I use string libraby to remove all kinds of punctuation but keep hashtag word. <br>\n",
    "ex: #sad -> sad <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T06:11:03.353711Z",
     "start_time": "2019-11-27T06:11:00.607573Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "translator = str.maketrans('','',string.punctuation)\n",
    "trains['cleaned'] = trains['lemmazation'].str.replace('[{}]'.format(string.punctuation), '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T06:11:03.366182Z",
     "start_time": "2019-11-27T06:11:03.354949Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>emotion</th>\n",
       "      <th>identification</th>\n",
       "      <th>stop_words</th>\n",
       "      <th>sentimentText</th>\n",
       "      <th>stemming</th>\n",
       "      <th>lemmazation</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>People post \"add #Snapchat\" must dehydrated. C...</td>\n",
       "      <td>People post \"add #Snapchat\" must dehydrated. C...</td>\n",
       "      <td>peopl post \"add #snapchat\" must dehydrated. cu...</td>\n",
       "      <td>peopl post \"add #snapchat\" must dehydrated. cu...</td>\n",
       "      <td>[peopl, post, add, snapchat, must, dehydrated,...</td>\n",
       "      <td>peopl post add snapchat must dehydrated cuz ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>sadness</td>\n",
       "      <td>train</td>\n",
       "      <td>@brianklaas As see, Trump dangerous #freepress...</td>\n",
       "      <td>@brianklaas As see, Trump dangerous #freepress...</td>\n",
       "      <td>@brianklaa As see, trump danger #freepress aro...</td>\n",
       "      <td>@brianklaa As see, trump danger #freepress aro...</td>\n",
       "      <td>[brianklaa, As, see, trump, danger, freepress,...</td>\n",
       "      <td>brianklaa As see trump danger freepress around...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>fear</td>\n",
       "      <td>train</td>\n",
       "      <td>Now ISSA stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>Now ISSA stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>now issa stalk tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;lh&gt;</td>\n",
       "      <td>now issa stalk tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;lh&gt;</td>\n",
       "      <td>[now, issa, stalk, tasha, ðŸ˜‚ðŸ˜‚ðŸ˜‚, lh]</td>\n",
       "      <td>now issa stalk tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ lh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx BEST TIME tonig...</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thank You BEST TIME...</td>\n",
       "      <td>@riskshow @thekevinallison thank you best time...</td>\n",
       "      <td>@riskshow @thekevinallison thank you best time...</td>\n",
       "      <td>[riskshow, thekevinallison, thank, you, best, ...</td>\n",
       "      <td>riskshow thekevinallison thank you best time t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>Still waiting supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>Still waiting supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>still wait suppli liscus. &lt;lh&gt;</td>\n",
       "      <td>still wait suppli liscus. &lt;lh&gt;</td>\n",
       "      <td>[still, wait, suppli, liscus, lh]</td>\n",
       "      <td>still wait suppli liscus lh</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                               text  \\\n",
       "0  0x376b20  People who post \"add me on #Snapchat\" must be ...   \n",
       "1  0x2d5350  @brianklaas As we see, Trump is dangerous to #...   \n",
       "3  0x1cd5b0                Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>   \n",
       "5  0x1d755c  @RISKshow @TheKevinAllison Thx for the BEST TI...   \n",
       "6  0x2c91a8       Still waiting on those supplies Liscus. <LH>   \n",
       "\n",
       "                        hashtags       emotion identification  \\\n",
       "0                     [Snapchat]  anticipation          train   \n",
       "1  [freepress, TrumpLegacy, CNN]       sadness          train   \n",
       "3                             []          fear          train   \n",
       "5      [authentic, LaughOutLoud]           joy          train   \n",
       "6                             []  anticipation          train   \n",
       "\n",
       "                                          stop_words  \\\n",
       "0  People post \"add #Snapchat\" must dehydrated. C...   \n",
       "1  @brianklaas As see, Trump dangerous #freepress...   \n",
       "3                   Now ISSA stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>   \n",
       "5  @RISKshow @TheKevinAllison Thx BEST TIME tonig...   \n",
       "6                Still waiting supplies Liscus. <LH>   \n",
       "\n",
       "                                       sentimentText  \\\n",
       "0  People post \"add #Snapchat\" must dehydrated. C...   \n",
       "1  @brianklaas As see, Trump dangerous #freepress...   \n",
       "3                   Now ISSA stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>   \n",
       "5  @RISKshow @TheKevinAllison Thank You BEST TIME...   \n",
       "6                Still waiting supplies Liscus. <LH>   \n",
       "\n",
       "                                            stemming  \\\n",
       "0  peopl post \"add #snapchat\" must dehydrated. cu...   \n",
       "1  @brianklaa As see, trump danger #freepress aro...   \n",
       "3                      now issa stalk tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <lh>   \n",
       "5  @riskshow @thekevinallison thank you best time...   \n",
       "6                     still wait suppli liscus. <lh>   \n",
       "\n",
       "                                         lemmazation  \\\n",
       "0  peopl post \"add #snapchat\" must dehydrated. cu...   \n",
       "1  @brianklaa As see, trump danger #freepress aro...   \n",
       "3                      now issa stalk tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <lh>   \n",
       "5  @riskshow @thekevinallison thank you best time...   \n",
       "6                     still wait suppli liscus. <lh>   \n",
       "\n",
       "                                      text_tokenized  \\\n",
       "0  [peopl, post, add, snapchat, must, dehydrated,...   \n",
       "1  [brianklaa, As, see, trump, danger, freepress,...   \n",
       "3                 [now, issa, stalk, tasha, ðŸ˜‚ðŸ˜‚ðŸ˜‚, lh]   \n",
       "5  [riskshow, thekevinallison, thank, you, best, ...   \n",
       "6                  [still, wait, suppli, liscus, lh]   \n",
       "\n",
       "                                             cleaned  \n",
       "0  peopl post add snapchat must dehydrated cuz ma...  \n",
       "1  brianklaa As see trump danger freepress around...  \n",
       "3                        now issa stalk tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ lh  \n",
       "5  riskshow thekevinallison thank you best time t...  \n",
       "6                        still wait suppli liscus lh  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trains.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6). Related preprocessing method I tried"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried many different preprocessing method, and the below are what I tried but didn't use finally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " text_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T06:11:00.606513Z",
     "start_time": "2019-11-27T06:09:19.380815Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>lemmazation</th>\n",
       "      <th>text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>peopl post \"add #snapchat\" must dehydrated. cu...</td>\n",
       "      <td>[peopl, post, add, snapchat, must, dehydrated,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaa As see, trump danger #freepress aro...</td>\n",
       "      <td>[brianklaa, As, see, trump, danger, freepress,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>now issa stalk tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;lh&gt;</td>\n",
       "      <td>[now, issa, stalk, tasha, ðŸ˜‚ðŸ˜‚ðŸ˜‚, lh]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>@riskshow @thekevinallison thank you best time...</td>\n",
       "      <td>[riskshow, thekevinallison, thank, you, best, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>still wait suppli liscus. &lt;lh&gt;</td>\n",
       "      <td>[still, wait, suppli, liscus, lh]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                        lemmazation  \\\n",
       "0  0x376b20  peopl post \"add #snapchat\" must dehydrated. cu...   \n",
       "1  0x2d5350  @brianklaa As see, trump danger #freepress aro...   \n",
       "3  0x1cd5b0                      now issa stalk tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <lh>   \n",
       "5  0x1d755c  @riskshow @thekevinallison thank you best time...   \n",
       "6  0x2c91a8                     still wait suppli liscus. <lh>   \n",
       "\n",
       "                                      text_tokenized  \n",
       "0  [peopl, post, add, snapchat, must, dehydrated,...  \n",
       "1  [brianklaa, As, see, trump, danger, freepress,...  \n",
       "3                 [now, issa, stalk, tasha, ðŸ˜‚ðŸ˜‚ðŸ˜‚, lh]  \n",
       "5  [riskshow, thekevinallison, thank, you, best, ...  \n",
       "6                  [still, wait, suppli, liscus, lh]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text_tokenized from texts to lists of words\n",
    "import gensim\n",
    "\n",
    "## ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "translator = str.maketrans('','',string.punctuation)\n",
    "\n",
    "trains['text_tokenized'] = trains['lemmazation'].apply(lambda x: nltk.word_tokenize(x.translate(translator)))\n",
    "trains[['tweet_id', 'lemmazation', 'text_tokenized']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change emoji to words based on emoji library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T06:35:59.813216Z",
     "start_time": "2019-11-28T06:33:21.375305Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>lemmazation</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>c1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>peopl post \"add #snapchat\" must dehydrated. cu...</td>\n",
       "      <td>[peopl, post, add, snapchat, must, dehydrated,...</td>\n",
       "      <td>peopl post add snapchat must dehydrated cuz ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaa As see, trump danger #freepress aro...</td>\n",
       "      <td>[brianklaa, As, see, trump, danger, freepress,...</td>\n",
       "      <td>brianklaa As see trump danger freepress around...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>now issa stalk tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;lh&gt;</td>\n",
       "      <td>[now, issa, stalk, tasha, ðŸ˜‚ðŸ˜‚ðŸ˜‚, lh]</td>\n",
       "      <td>now issa stalk tasha :face_with_tears_of_joy::...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>@riskshow @thekevinallison thank you best time...</td>\n",
       "      <td>[riskshow, thekevinallison, thank, you, best, ...</td>\n",
       "      <td>riskshow thekevinallison thank you best time t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>still wait suppli liscus. &lt;lh&gt;</td>\n",
       "      <td>[still, wait, suppli, liscus, lh]</td>\n",
       "      <td>still wait suppli liscus lh</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                        lemmazation  \\\n",
       "0  0x376b20  peopl post \"add #snapchat\" must dehydrated. cu...   \n",
       "1  0x2d5350  @brianklaa As see, trump danger #freepress aro...   \n",
       "3  0x1cd5b0                      now issa stalk tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <lh>   \n",
       "5  0x1d755c  @riskshow @thekevinallison thank you best time...   \n",
       "6  0x2c91a8                     still wait suppli liscus. <lh>   \n",
       "\n",
       "                                      text_tokenized  \\\n",
       "0  [peopl, post, add, snapchat, must, dehydrated,...   \n",
       "1  [brianklaa, As, see, trump, danger, freepress,...   \n",
       "3                 [now, issa, stalk, tasha, ðŸ˜‚ðŸ˜‚ðŸ˜‚, lh]   \n",
       "5  [riskshow, thekevinallison, thank, you, best, ...   \n",
       "6                  [still, wait, suppli, liscus, lh]   \n",
       "\n",
       "                                                  c1  \n",
       "0  peopl post add snapchat must dehydrated cuz ma...  \n",
       "1  brianklaa As see trump danger freepress around...  \n",
       "3  now issa stalk tasha :face_with_tears_of_joy::...  \n",
       "5  riskshow thekevinallison thank you best time t...  \n",
       "6                        still wait suppli liscus lh  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import emoji\n",
    "trains['c1'] = trains['cleaned'].apply(lambda x: (emoji.demojize(x)))\n",
    "trains[['tweet_id', 'lemmazation', 'text_tokenized', 'c1']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T06:36:01.264044Z",
     "start_time": "2019-11-28T06:35:59.814144Z"
    }
   },
   "outputs": [],
   "source": [
    "translator = str.maketrans('','',string.punctuation)\n",
    "trains['c2'] = trains['c1'].str.replace('[{}]'.format(string.punctuation), '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T06:36:01.923743Z",
     "start_time": "2019-11-28T06:36:01.265217Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>lemmazation</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>c2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>peopl post \"add #snapchat\" must dehydrated. cu...</td>\n",
       "      <td>[peopl, post, add, snapchat, must, dehydrated,...</td>\n",
       "      <td>peopl post add snapchat must dehydrated cuz ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaa As see, trump danger #freepress aro...</td>\n",
       "      <td>[brianklaa, As, see, trump, danger, freepress,...</td>\n",
       "      <td>brianklaa As see trump danger freepress around...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>now issa stalk tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;lh&gt;</td>\n",
       "      <td>[now, issa, stalk, tasha, ðŸ˜‚ðŸ˜‚ðŸ˜‚, lh]</td>\n",
       "      <td>now issa stalk tasha facewithtearsofjoyfacewit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>@riskshow @thekevinallison thank you best time...</td>\n",
       "      <td>[riskshow, thekevinallison, thank, you, best, ...</td>\n",
       "      <td>riskshow thekevinallison thank you best time t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>still wait suppli liscus. &lt;lh&gt;</td>\n",
       "      <td>[still, wait, suppli, liscus, lh]</td>\n",
       "      <td>still wait suppli liscus lh</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                        lemmazation  \\\n",
       "0  0x376b20  peopl post \"add #snapchat\" must dehydrated. cu...   \n",
       "1  0x2d5350  @brianklaa As see, trump danger #freepress aro...   \n",
       "3  0x1cd5b0                      now issa stalk tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <lh>   \n",
       "5  0x1d755c  @riskshow @thekevinallison thank you best time...   \n",
       "6  0x2c91a8                     still wait suppli liscus. <lh>   \n",
       "\n",
       "                                      text_tokenized  \\\n",
       "0  [peopl, post, add, snapchat, must, dehydrated,...   \n",
       "1  [brianklaa, As, see, trump, danger, freepress,...   \n",
       "3                 [now, issa, stalk, tasha, ðŸ˜‚ðŸ˜‚ðŸ˜‚, lh]   \n",
       "5  [riskshow, thekevinallison, thank, you, best, ...   \n",
       "6                  [still, wait, suppli, liscus, lh]   \n",
       "\n",
       "                                                  c2  \n",
       "0  peopl post add snapchat must dehydrated cuz ma...  \n",
       "1  brianklaa As see trump danger freepress around...  \n",
       "3  now issa stalk tasha facewithtearsofjoyfacewit...  \n",
       "5  riskshow thekevinallison thank you best time t...  \n",
       "6                        still wait suppli liscus lh  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trains[['tweet_id', 'lemmazation', 'text_tokenized', 'c2']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (7). Our new dataframe after preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T11:59:27.447288Z",
     "start_time": "2019-11-27T11:59:27.248842Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>emotion</th>\n",
       "      <th>identification</th>\n",
       "      <th>stop_words</th>\n",
       "      <th>sentimentText</th>\n",
       "      <th>stemming</th>\n",
       "      <th>lemmazation</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>People post \"add #Snapchat\" must dehydrated. C...</td>\n",
       "      <td>People post \"add #Snapchat\" must dehydrated. C...</td>\n",
       "      <td>peopl post \"add #snapchat\" must dehydrated. cu...</td>\n",
       "      <td>peopl post \"add #snapchat\" must dehydrated. cu...</td>\n",
       "      <td>[peopl, post, add, snapchat, must, dehydrated,...</td>\n",
       "      <td>peopl post add snapchat must dehydrated cuz ma...</td>\n",
       "      <td>peopl post add snapchat must dehydrated cuz ma...</td>\n",
       "      <td>peopl post add snapchat must dehydrated cuz ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>sadness</td>\n",
       "      <td>train</td>\n",
       "      <td>@brianklaas As see, Trump dangerous #freepress...</td>\n",
       "      <td>@brianklaas As see, Trump dangerous #freepress...</td>\n",
       "      <td>@brianklaa As see, trump danger #freepress aro...</td>\n",
       "      <td>@brianklaa As see, trump danger #freepress aro...</td>\n",
       "      <td>[brianklaa, As, see, trump, danger, freepress,...</td>\n",
       "      <td>brianklaa As see trump danger freepress around...</td>\n",
       "      <td>brianklaa As see trump danger freepress around...</td>\n",
       "      <td>brianklaa As see trump danger freepress around...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>fear</td>\n",
       "      <td>train</td>\n",
       "      <td>Now ISSA stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>Now ISSA stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;LH&gt;</td>\n",
       "      <td>now issa stalk tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;lh&gt;</td>\n",
       "      <td>now issa stalk tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ &lt;lh&gt;</td>\n",
       "      <td>[now, issa, stalk, tasha, ðŸ˜‚ðŸ˜‚ðŸ˜‚, lh]</td>\n",
       "      <td>now issa stalk tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ lh</td>\n",
       "      <td>now issa stalk tasha :face_with_tears_of_joy::...</td>\n",
       "      <td>now issa stalk tasha facewithtearsofjoyfacewit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx BEST TIME tonig...</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thank You BEST TIME...</td>\n",
       "      <td>@riskshow @thekevinallison thank you best time...</td>\n",
       "      <td>@riskshow @thekevinallison thank you best time...</td>\n",
       "      <td>[riskshow, thekevinallison, thank, you, best, ...</td>\n",
       "      <td>riskshow thekevinallison thank you best time t...</td>\n",
       "      <td>riskshow thekevinallison thank you best time t...</td>\n",
       "      <td>riskshow thekevinallison thank you best time t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>Still waiting supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>Still waiting supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>still wait suppli liscus. &lt;lh&gt;</td>\n",
       "      <td>still wait suppli liscus. &lt;lh&gt;</td>\n",
       "      <td>[still, wait, suppli, liscus, lh]</td>\n",
       "      <td>still wait suppli liscus lh</td>\n",
       "      <td>still wait suppli liscus lh</td>\n",
       "      <td>still wait suppli liscus lh</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                               text  \\\n",
       "0  0x376b20  People who post \"add me on #Snapchat\" must be ...   \n",
       "1  0x2d5350  @brianklaas As we see, Trump is dangerous to #...   \n",
       "3  0x1cd5b0                Now ISSA is stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>   \n",
       "5  0x1d755c  @RISKshow @TheKevinAllison Thx for the BEST TI...   \n",
       "6  0x2c91a8       Still waiting on those supplies Liscus. <LH>   \n",
       "\n",
       "                        hashtags       emotion identification  \\\n",
       "0                     [Snapchat]  anticipation          train   \n",
       "1  [freepress, TrumpLegacy, CNN]       sadness          train   \n",
       "3                             []          fear          train   \n",
       "5      [authentic, LaughOutLoud]           joy          train   \n",
       "6                             []  anticipation          train   \n",
       "\n",
       "                                          stop_words  \\\n",
       "0  People post \"add #Snapchat\" must dehydrated. C...   \n",
       "1  @brianklaas As see, Trump dangerous #freepress...   \n",
       "3                   Now ISSA stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>   \n",
       "5  @RISKshow @TheKevinAllison Thx BEST TIME tonig...   \n",
       "6                Still waiting supplies Liscus. <LH>   \n",
       "\n",
       "                                       sentimentText  \\\n",
       "0  People post \"add #Snapchat\" must dehydrated. C...   \n",
       "1  @brianklaas As see, Trump dangerous #freepress...   \n",
       "3                   Now ISSA stalking Tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <LH>   \n",
       "5  @RISKshow @TheKevinAllison Thank You BEST TIME...   \n",
       "6                Still waiting supplies Liscus. <LH>   \n",
       "\n",
       "                                            stemming  \\\n",
       "0  peopl post \"add #snapchat\" must dehydrated. cu...   \n",
       "1  @brianklaa As see, trump danger #freepress aro...   \n",
       "3                      now issa stalk tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <lh>   \n",
       "5  @riskshow @thekevinallison thank you best time...   \n",
       "6                     still wait suppli liscus. <lh>   \n",
       "\n",
       "                                         lemmazation  \\\n",
       "0  peopl post \"add #snapchat\" must dehydrated. cu...   \n",
       "1  @brianklaa As see, trump danger #freepress aro...   \n",
       "3                      now issa stalk tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ <lh>   \n",
       "5  @riskshow @thekevinallison thank you best time...   \n",
       "6                     still wait suppli liscus. <lh>   \n",
       "\n",
       "                                      text_tokenized  \\\n",
       "0  [peopl, post, add, snapchat, must, dehydrated,...   \n",
       "1  [brianklaa, As, see, trump, danger, freepress,...   \n",
       "3                 [now, issa, stalk, tasha, ðŸ˜‚ðŸ˜‚ðŸ˜‚, lh]   \n",
       "5  [riskshow, thekevinallison, thank, you, best, ...   \n",
       "6                  [still, wait, suppli, liscus, lh]   \n",
       "\n",
       "                                             cleaned  \\\n",
       "0  peopl post add snapchat must dehydrated cuz ma...   \n",
       "1  brianklaa As see trump danger freepress around...   \n",
       "3                        now issa stalk tasha ðŸ˜‚ðŸ˜‚ðŸ˜‚ lh   \n",
       "5  riskshow thekevinallison thank you best time t...   \n",
       "6                        still wait suppli liscus lh   \n",
       "\n",
       "                                                  c1  \\\n",
       "0  peopl post add snapchat must dehydrated cuz ma...   \n",
       "1  brianklaa As see trump danger freepress around...   \n",
       "3  now issa stalk tasha :face_with_tears_of_joy::...   \n",
       "5  riskshow thekevinallison thank you best time t...   \n",
       "6                        still wait suppli liscus lh   \n",
       "\n",
       "                                                  c2  \n",
       "0  peopl post add snapchat must dehydrated cuz ma...  \n",
       "1  brianklaa As see trump danger freepress around...  \n",
       "3  now issa stalk tasha facewithtearsofjoyfacewit...  \n",
       "5  riskshow thekevinallison thank you best time t...  \n",
       "6                        still wait suppli liscus lh  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trains.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can save them as pickle file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T06:12:26.084587Z",
     "start_time": "2019-11-27T06:12:19.453184Z"
    }
   },
   "outputs": [],
   "source": [
    "## save to pickle file\n",
    "trains.to_pickle(\"train_cleaned.pkl\") \n",
    "tests.to_pickle(\"test_cleaned.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T06:33:07.869875Z",
     "start_time": "2019-11-28T06:33:01.790148Z"
    }
   },
   "outputs": [],
   "source": [
    "## load a pickle file\n",
    "trains = pd.read_pickle(\"train_cleaned.pkl\")\n",
    "tests = pd.read_pickle(\"test_cleaned.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2-1. feature engineering steps - TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In feature engineering steps, I have tried TF-IDF and BOW to select my feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T11:59:46.038406Z",
     "start_time": "2019-11-27T11:59:46.002071Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1164450, 13)\n",
      "(1164450,)\n",
      "(291113, 13)\n",
      "(291113,)\n"
     ]
    }
   ],
   "source": [
    "# seperate training and validation data\n",
    "\n",
    "sep = int(trains.shape[0] * 0.8) # 20% for validation\n",
    "\n",
    "BOW_train = trains[:sep]\n",
    "BOW_train_label = trains.emotion[:sep]\n",
    "BOW_test = trains[sep:]\n",
    "BOW_test_label = trains.emotion[sep:]\n",
    "\n",
    "print(BOW_train.shape)\n",
    "print(BOW_train_label.shape)\n",
    "print(BOW_test.shape)\n",
    "print(BOW_test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T08:01:38.408299Z",
     "start_time": "2019-11-27T08:01:27.686483Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=1500, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# TF-IDF feature selecting\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1500, use_idf=True)\n",
    "tfidf_vectorizer.fit(BOW_train['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T06:15:15.865671Z",
     "start_time": "2019-11-27T06:14:38.148217Z"
    }
   },
   "outputs": [],
   "source": [
    "BOW_train = BOW.transform(trains['cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T06:38:38.572327Z",
     "start_time": "2019-11-28T06:38:38.561905Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yield',\n",
       " 'yikes',\n",
       " 'ynwa',\n",
       " 'yo',\n",
       " 'yoga',\n",
       " 'yogi',\n",
       " 'yolo',\n",
       " 'york',\n",
       " 'yorkshir',\n",
       " 'you',\n",
       " 'youcandoit',\n",
       " 'youd',\n",
       " 'youll',\n",
       " 'young',\n",
       " 'younger',\n",
       " 'youngest',\n",
       " 'your',\n",
       " 'youredheart',\n",
       " 'yourock',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'youth',\n",
       " 'youthinkingface',\n",
       " 'youtub',\n",
       " 'youtube',\n",
       " 'youu',\n",
       " 'youv',\n",
       " 'yr',\n",
       " 'yrs',\n",
       " 'yt',\n",
       " 'yu',\n",
       " 'yuck',\n",
       " 'yum',\n",
       " 'yummi',\n",
       " 'yup',\n",
       " 'yvonneorji',\n",
       " 'yyc',\n",
       " 'z',\n",
       " 'z100newyork',\n",
       " 'z1043',\n",
       " 'zach',\n",
       " 'zakbagan',\n",
       " 'zakhmi',\n",
       " 'zanyface',\n",
       " 'zapti',\n",
       " 'zaynmalik',\n",
       " 'zbczone',\n",
       " 'zealand',\n",
       " 'zeke',\n",
       " 'zen',\n",
       " 'zero',\n",
       " 'zimbabw',\n",
       " 'zion',\n",
       " 'zip',\n",
       " 'zombi',\n",
       " 'zone',\n",
       " 'zoo',\n",
       " 'zzz',\n",
       " '\\x92',\n",
       " 'Â¡',\n",
       " 'Â£',\n",
       " 'Â«',\n",
       " 'Â¯',\n",
       " 'Â°',\n",
       " 'Â´',\n",
       " 'Â·',\n",
       " 'Â»',\n",
       " 'Â¿',\n",
       " 'Ã—',\n",
       " 'à¤¾',\n",
       " 'à¥€',\n",
       " 'à¥',\n",
       " 'áƒ¦',\n",
       " '\\u200b',\n",
       " 'â€“',\n",
       " 'â€”',\n",
       " 'â€•',\n",
       " 'â€˜',\n",
       " 'â€™',\n",
       " 'â€œ',\n",
       " 'â€',\n",
       " 'â€¢',\n",
       " 'â€¦',\n",
       " 'â‚¬',\n",
       " 'â”',\n",
       " 'â”ƒ',\n",
       " 'â”ˆ',\n",
       " 'â–ˆ',\n",
       " 'â–‘',\n",
       " 'â˜…',\n",
       " 'â˜†',\n",
       " 'â™¡',\n",
       " 'â™ª',\n",
       " 'â™«',\n",
       " 'âš˜',\n",
       " 'ï¿½',\n",
       " 'ðŸ•†',\n",
       " 'ðŸ•‡',\n",
       " 'ðŸ–’']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.get_feature_names()[-100:] # show features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T08:11:55.241509Z",
     "start_time": "2019-11-27T08:11:55.234460Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW_train_X.shape:  (1164450, 11)\n",
      "BOW_train_y.shape:  (1164450,)\n",
      "BOW_test_X.shape:  (0, 11)\n",
      "BOW_test_y.shape:  (291113,)\n"
     ]
    }
   ],
   "source": [
    "sep = int(trains.shape[0] * 0.8) # 20% for validation\n",
    "\n",
    "\n",
    "BOW_train_X = BOW_train[:sep]\n",
    "BOW_train_y = trains.emotion[:sep]\n",
    "\n",
    "BOW_test_X = BOW_train[sep:]\n",
    "BOW_test_y = trains.emotion[sep:]\n",
    "\n",
    "## take a look at data dimension is a good habbit  :)\n",
    "print('BOW_train_X.shape: ', BOW_train_X.shape)\n",
    "print('BOW_train_y.shape: ', BOW_train_y.shape)\n",
    "print('BOW_test_X.shape: ', BOW_test_X.shape)\n",
    "print('BOW_test_y.shape: ', BOW_test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T08:12:03.385917Z",
     "start_time": "2019-11-27T08:12:03.354445Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check label:  ['anger' 'anticipation' 'disgust' 'fear' 'joy' 'sadness' 'surprise'\n",
      " 'trust']\n",
      "\n",
      "## Before convert\n",
      "y_train[0:7]:\n",
      " 0    anticipation\n",
      "1         sadness\n",
      "3            fear\n",
      "5             joy\n",
      "6    anticipation\n",
      "7             joy\n",
      "8         sadness\n",
      "Name: emotion, dtype: object\n",
      "\n",
      "y_train.shape:  (1164450,)\n",
      "y_test.shape:  (291113,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(BOW_train_y)\n",
    "print('check label: ', label_encoder.classes_)\n",
    "print('\\n## Before convert')\n",
    "print('y_train[0:7]:\\n', BOW_train_y[0:7])\n",
    "print('\\ny_train.shape: ',BOW_train_y.shape)\n",
    "print('y_test.shape: ', BOW_test_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T08:12:06.904065Z",
     "start_time": "2019-11-27T08:12:06.768411Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "## After convert\n",
      "y_train[0:7]:\n",
      " [[0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "\n",
      "y_train.shape:  (1164450, 8)\n",
      "y_test.shape:  (291113, 8)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "def label_encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return keras.utils.to_categorical(enc)\n",
    "\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "BOW_train_y = label_encode(label_encoder, BOW_train_y)\n",
    "BOW_test_y = label_encode(label_encoder, BOW_test_y)\n",
    "\n",
    "print('\\n\\n## After convert')\n",
    "print('y_train[0:7]:\\n', BOW_train_y[0:7])\n",
    "print('\\ny_train.shape: ', BOW_train_y.shape)\n",
    "print('y_test.shape: ', BOW_test_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T08:12:09.789328Z",
     "start_time": "2019-11-27T08:12:09.783532Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape:  11\n",
      "output_shape:  8\n"
     ]
    }
   ],
   "source": [
    "# I/O check\n",
    "input_shape = BOW_train_X.shape[1]\n",
    "print('input_shape: ', input_shape)\n",
    "\n",
    "output_shape = len(label_encoder.classes_)\n",
    "print('output_shape: ', output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3-1. Deep learning Model - TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T06:22:10.804447Z",
     "start_time": "2019-11-27T06:22:10.510071Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3000)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1536512   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "re_lu_4 (ReLU)               (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "re_lu_5 (ReLU)               (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 8)                 264       \n",
      "_________________________________________________________________\n",
      "softmax_1 (Softmax)          (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 1,694,504\n",
      "Trainable params: 1,692,648\n",
      "Non-trainable params: 1,856\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers import ReLU, Softmax\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "\n",
    "# input layer\n",
    "model_input = Input(shape=(input_shape, ))  # 3000\n",
    "X = model_input\n",
    "\n",
    "# 1st hidden layer\n",
    "\n",
    "X_W1 = Dense(units=512, kernel_regularizer = regularizers.l2(0.001), activity_regularizer=regularizers.l1(0.01))(X)  # 64\n",
    "H1 = BatchNormalization()(X_W1)\n",
    "H1 = ReLU()(H1)\n",
    "H1_Drop = Dropout(0.4)(H1)\n",
    "\n",
    "# 2nd hidden layer\n",
    "X_W2 = Dense(units=256, kernel_regularizer = regularizers.l2(0.001), activity_regularizer=regularizers.l1(0.01))(H1_Drop)  # 64\n",
    "H2 = BatchNormalization()(X_W2)\n",
    "H2 = ReLU()(H2)\n",
    "H2_Drop = Dropout(0.4)(H2)\n",
    "\n",
    "\n",
    "# 3rd hidden layer\n",
    "X_W3 = Dense(units=64, kernel_regularizer = regularizers.l2(0.001), activity_regularizer=regularizers.l1(0.01))(H2_Drop)  # 64\n",
    "H3 = BatchNormalization()(X_W3)\n",
    "H3 = ReLU()(H3)\n",
    "H3_Drop = Dropout(0.4)(H3)\n",
    "\n",
    "# 4th hidden layer\n",
    "X_W4 = Dense(units=64, kernel_regularizer = regularizers.l2(0.001), activity_regularizer=regularizers.l1(0.01))(H3_Drop)  # 64\n",
    "H4 = BatchNormalization()(X_W4)\n",
    "H4 = ReLU()(H4)\n",
    "H4_Drop = Dropout(0.4)(H4)\n",
    "\n",
    "# 5th hidden layer\n",
    "X_W5 = Dense(units=32, kernel_regularizer = regularizers.l2(0.001), activity_regularizer=regularizers.l1(0.01))(H4_Drop)  # 64\n",
    "H5 = BatchNormalization()(X_W5)\n",
    "H5 = ReLU()(H5)\n",
    "H5_Drop = Dropout(0.4)(H5)\n",
    "\n",
    "# output layer\n",
    "X_H6 = Dense(units=output_shape)(H5_Drop)  # 4\n",
    "H6 = Softmax()(X_H6)\n",
    "\n",
    "model_output = H6\n",
    "\n",
    "# create model\n",
    "model = Model(inputs=[model_input], outputs=[model_output])\n",
    "\n",
    "# loss function & optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# show model construction\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "csv_logger = CSVLogger('logs/training_log_BOW.csv')\n",
    "\n",
    "# training setting\n",
    "epochs = 25\n",
    "batch_size = 32\n",
    "\n",
    "# training!\n",
    "history = model.fit(BOW_train_X, BOW_train_y, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=[csv_logger],\n",
    "                    validation_data = (BOW_test_X, BOW_test_y))\n",
    "print('training finish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2-2. feature engineering steps - BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T06:36:24.094671Z",
     "start_time": "2019-11-28T06:36:24.088436Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1164450, 13)\n",
      "(1164450,)\n",
      "(291113, 13)\n",
      "(291113,)\n"
     ]
    }
   ],
   "source": [
    "sep = int(trains.shape[0] * 0.8) # 20% for validation\n",
    "\n",
    "BOW_train = trains[:sep]\n",
    "BOW_train_label = trains.emotion[:sep]\n",
    "BOW_test = trains[sep:]\n",
    "BOW_test_label = trains.emotion[sep:]\n",
    "\n",
    "print(BOW_train.shape)\n",
    "print(BOW_train_label.shape)\n",
    "print(BOW_test.shape)\n",
    "print(BOW_test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T06:14:38.147201Z",
     "start_time": "2019-11-27T06:13:58.169849Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.8, max_features=3000, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7f1a69508940>>,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bag of word feature selecting\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "# build analyzers (bag-of-words)\n",
    "BOW = CountVectorizer(analyzer='word', max_features=3000, max_df=0.8, tokenizer=tknzr.tokenize)\n",
    "\n",
    "# apply analyzer to training data\n",
    "BOW.fit(trains['cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T06:46:26.577107Z",
     "start_time": "2019-11-28T06:45:50.101241Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (1164450, 5000)\n",
      "y_train.shape:  (1164450,)\n",
      "X_test.shape:  (291113, 5000)\n",
      "y_test.shape:  (291113,)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "# standardize name (X, y) \n",
    "X_train = tfidf_vectorizer.transform(BOW_train['c2'])\n",
    "y_train = BOW_train_label\n",
    "\n",
    "X_test = tfidf_vectorizer.transform(BOW_test['c2'])\n",
    "y_test = BOW_test_label\n",
    "\n",
    "## check dimension is a good habbit \n",
    "print('X_train.shape: ', X_train.shape)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('X_test.shape: ', X_test.shape)\n",
    "print('y_test.shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T06:46:26.606564Z",
     "start_time": "2019-11-28T06:46:26.578304Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check label:  ['anger' 'anticipation' 'disgust' 'fear' 'joy' 'sadness' 'surprise'\n",
      " 'trust']\n",
      "\n",
      "## Before convert\n",
      "y_train[0:7]:\n",
      " 0    anticipation\n",
      "1         sadness\n",
      "3            fear\n",
      "5             joy\n",
      "6    anticipation\n",
      "7             joy\n",
      "8         sadness\n",
      "Name: emotion, dtype: object\n",
      "\n",
      "y_train.shape:  (1164450,)\n",
      "y_test.shape:  (291113,)\n"
     ]
    }
   ],
   "source": [
    "## deal with label (string -> one-hot)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "print('check label: ', label_encoder.classes_)\n",
    "print('\\n## Before convert')\n",
    "print('y_train[0:7]:\\n', y_train[0:7])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "print('y_test.shape: ', y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T06:46:26.736918Z",
     "start_time": "2019-11-28T06:46:26.607706Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "## After convert\n",
      "y_train[0:7]:\n",
      " [[0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "\n",
      "y_train.shape:  (1164450, 8)\n",
      "y_test.shape:  (291113, 8)\n"
     ]
    }
   ],
   "source": [
    "def label_encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return keras.utils.to_categorical(enc)\n",
    "\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "y_train = label_encode(label_encoder, y_train)\n",
    "y_test = label_encode(label_encoder, y_test)\n",
    "\n",
    "print('\\n\\n## After convert')\n",
    "print('y_train[0:7]:\\n', y_train[0:7])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "print('y_test.shape: ', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T06:46:26.740448Z",
     "start_time": "2019-11-28T06:46:26.738024Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape:  5000\n",
      "output_shape:  8\n"
     ]
    }
   ],
   "source": [
    "# I/O check\n",
    "input_shape = X_train.shape[1]\n",
    "print('input_shape: ', input_shape)\n",
    "\n",
    "output_shape = len(label_encoder.classes_)\n",
    "print('output_shape: ', output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3-2. Deep learning model - BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T06:46:40.792536Z",
     "start_time": "2019-11-28T06:46:40.732057Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 5000)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                320064    \n",
      "_________________________________________________________________\n",
      "re_lu_7 (ReLU)               (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "re_lu_8 (ReLU)               (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 8)                 520       \n",
      "_________________________________________________________________\n",
      "softmax_4 (Softmax)          (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 324,744\n",
      "Trainable params: 324,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers import ReLU, Softmax\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# input layer\n",
    "model_input = Input(shape=(input_shape, ))  # 10000\n",
    "X = model_input\n",
    "\n",
    "# 1st hidden layer\n",
    "\n",
    "\n",
    "\n",
    "X_W1 = Dense(units=64)(X)  # 64\n",
    "H1 = ReLU()(X_W1)\n",
    "\n",
    "# 2nd hidden layer\n",
    "H1_W2 = Dense(units=64)(H1)  # 64\n",
    "H2 = ReLU()(H1_W2)\n",
    "\n",
    "# H2_W3 = Dense(units=64)(H2)  # 64\n",
    "# H3 = ReLU()(H2_W3)\n",
    "\n",
    "# H3_W4 = Dense(units=64)(H3)  # 64\n",
    "# H4 = ReLU()(H3_W4)\n",
    "\n",
    "# H4_W5 = Dense(units=64)(H4)  # 64\n",
    "# H5 = ReLU()(H4_W5)\n",
    "\n",
    "# H5_W6 = Dense(units=64)(H5)  # 64\n",
    "# H6 = ReLU()(H5_W6)\n",
    "\n",
    "# output layer\n",
    "H2_W3 = Dense(units=output_shape)(H2)  # 8\n",
    "H3 = Softmax()(H2_W3)\n",
    "\n",
    "model_output = H3\n",
    "\n",
    "# create model\n",
    "model = Model(inputs=[model_input], outputs=[model_output])\n",
    "\n",
    "# loss function & optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# show model construction\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T06:51:06.384011Z",
     "start_time": "2019-11-28T06:46:46.148496Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1164450 samples, validate on 291113 samples\n",
      "Epoch 1/10\n",
      " 142200/1164450 [==>...........................] - ETA: 20s - loss: 1.5393 - accuracy: 0.444"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>limit_output extension: Maximum message size of 10000 exceeded with 10074 characters</b>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "csv_logger = CSVLogger('logs/training_log_1128.csv')\n",
    "\n",
    "# training setting\n",
    "epochs = 10\n",
    "batch_size = 300\n",
    "\n",
    "# training!\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=[csv_logger],\n",
    "                    validation_data = (X_test, y_test))\n",
    "print('training finish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Prediction Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T06:52:59.285373Z",
     "start_time": "2019-11-28T06:52:55.540313Z"
    }
   },
   "outputs": [],
   "source": [
    "## predict\n",
    "pred_result = model.predict(X_test, batch_size=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T06:53:00.426867Z",
     "start_time": "2019-11-28T06:53:00.207858Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing accuracy: 0.52\n"
     ]
    }
   ],
   "source": [
    "# show result of testing accuracy\n",
    "pred_result = label_decode(label_encoder, pred_result)\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('testing accuracy: {}'.format(round(accuracy_score(label_decode(label_encoder, y_test), pred_result), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T06:53:59.060855Z",
     "start_time": "2019-11-28T06:53:20.767829Z"
    }
   },
   "outputs": [],
   "source": [
    "testing = TfidfVectorizer(max_features=5000, use_idf=True, token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b', tokenizer=tknzr.tokenize)\n",
    "testing.fit(tests['text'])\n",
    "testing = testing.transform(tests['text'])\n",
    "results = model.predict(testing, batch_size=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T06:54:13.211838Z",
     "start_time": "2019-11-28T06:54:13.158090Z"
    }
   },
   "outputs": [],
   "source": [
    "results = label_decode(label_encoder, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T06:54:14.447741Z",
     "start_time": "2019-11-28T06:54:14.438444Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['disgust', 'anticipation', 'sadness', 'disgust', 'joy'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T06:54:15.946410Z",
     "start_time": "2019-11-28T06:54:15.934329Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2     0x28b412\n",
       "4     0x2de201\n",
       "9     0x218443\n",
       "30    0x2939d5\n",
       "33    0x26289a\n",
       "Name: tweet_id, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tests['tweet_id'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T06:54:17.592915Z",
     "start_time": "2019-11-28T06:54:17.591370Z"
    }
   },
   "outputs": [],
   "source": [
    "# store prediction to csv file\n",
    "pred = tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T06:54:18.930055Z",
     "start_time": "2019-11-28T06:54:18.922306Z"
    }
   },
   "outputs": [],
   "source": [
    "pred['emotion'] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T06:54:20.024871Z",
     "start_time": "2019-11-28T06:54:19.973362Z"
    }
   },
   "outputs": [],
   "source": [
    "pred = pred.drop('hashtags', axis=1)\n",
    "pred = pred.drop('text', axis=1)\n",
    "pred = pred.drop('identification', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T06:54:22.370938Z",
     "start_time": "2019-11-28T06:54:22.358224Z"
    }
   },
   "outputs": [],
   "source": [
    "pred.rename(columns={'tweet_id':'id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T06:54:23.758746Z",
     "start_time": "2019-11-28T06:54:23.750533Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x28b412</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x2de201</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0x218443</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0x2939d5</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0x26289a</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id       emotion\n",
       "2   0x28b412       disgust\n",
       "4   0x2de201  anticipation\n",
       "9   0x218443       sadness\n",
       "30  0x2939d5       disgust\n",
       "33  0x26289a           joy"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T06:54:25.907697Z",
     "start_time": "2019-11-28T06:54:25.594368Z"
    }
   },
   "outputs": [],
   "source": [
    "pred.to_csv('prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle compition model - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import re, csv\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from gensim.models import KeyedVectors   \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.layers import Input, Dense, ReLU, Softmax, LSTM, Dropout, Embedding, Bidirectional\n",
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.initializers import Constant\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing step (the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cleaned data\n",
    "trains = pd.read_pickle(\"train_cleaned.pkl\")\n",
    "tests = pd.read_pickle(\"test_cleaned.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trains.head() # show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. feature engineering steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, I use pretrained Word2Vec model(English Common Crawl Corpus) to be my feature choosing.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Word2vec Model for feature\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(\"model_ENC3.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And I use Tokenizer() to filter all punctuations, in order to make tf-idf more better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer initialize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tknr = Tokenizer(num_words=None, \n",
    "               filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~ ', \n",
    "               lower=True, \n",
    "               split=' ', \n",
    "               char_level=False, \n",
    "               oov_token=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using TFIDF to select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAX_WORD = 80000 # max feature selecting\n",
    "\n",
    "# TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=MAX_WORD, \n",
    "    use_idf=True, \n",
    "    token_pattern='(?u)\\\\b[^\\\\d\\\\W][^\\\\d\\\\W]+\\\\b', \n",
    "    )\n",
    "tfidf_vectorizer.fit(trains['cleaned'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use tf-idf results to construct a token_dictionay (contain all MAX_WORD features)<br>\n",
    "key: feature_name<br>\n",
    "value: index of feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dict = {}\n",
    "for idx, token in enumerate(tfidf_vectorizer.get_feature_names()):\n",
    "    token_dict[token] = idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we have to encode all features to (300,1) vector <br>\n",
    "because our model only accept fixed size of input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "\n",
    "# return a matrix which all elements are (300,1) vectors\n",
    "def word2vec_matrix():\n",
    "    matrix_emb = np.zeros((len(token_dict)+1, EMBEDDING_DIM)) # replace MAX_WORD to len(token_dict)\n",
    "    for word, idx in token_dict.items():\n",
    "        try:\n",
    "            vector =  word2vec_model.wv[word.lower()] # remember to use word.lower() to search for word2vec\n",
    "        except:\n",
    "            vector = np.zeros(300,) # if didn't find word vector, just keep vector to be all zero\n",
    "        if idx < MAX_WORD:\n",
    "            matrix_emb[idx] = np.array(vector)  # define\n",
    "    matrix_emb[MAX_WORD] = np.zeros(300,)     # end of vector \n",
    "    return matrix_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to deal with test_data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lmtzr = WordNetLemmatizer()\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "\n",
    "# preprocessing step\n",
    "def translator1(user_string):\n",
    "    user_string = user_string.split(\" \")\n",
    "    j = 0\n",
    "    for _str in user_string:\n",
    "        # File path which consists of Abbreviations.\n",
    "        fileName = \"slang.txt\"\n",
    "        # File Access mode [Read Mode]\n",
    "        accessMode = \"r\"\n",
    "        with open(fileName, accessMode) as myCSVfile:\n",
    "            # Reading file as CSV with delimiter as \"=\", so that abbreviation are stored in row[0] and phrases in row[1]\n",
    "            dataFromFile = csv.reader(myCSVfile, delimiter=\"=\")\n",
    "            # Removing Special Characters.\n",
    "            _str = re.sub('[^a-zA-Z0-9-_.]', '', _str)\n",
    "            for row in dataFromFile:\n",
    "                # Check if selected word matches short forms[LHS] in text file.\n",
    "                if _str.upper() == row[0]:\n",
    "                    # If match found replace it with its appropriate phrase in text file.\n",
    "                    user_string[j] = row[1]\n",
    "        j = j + 1\n",
    "    # Replacing commas with spaces for final output.\n",
    "    return (' '.join(user_string))\n",
    "\n",
    "\n",
    "# do all my preprocessing steps again\n",
    "def cleaning(texts):\n",
    "    \n",
    "    \n",
    "    stop = stopwords.words('english')\n",
    "    texts = texts.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "    \n",
    "    texts = texts.apply(lambda x: translator1(x))\n",
    "    texts = texts.apply(lambda x: ' '.join([ps.stem(word) for word in x.split()]))\n",
    "    texts = texts.apply(lambda x: ' '.join([lmtzr.lemmatize(word,'v') for word in x.split()]))\n",
    "    translator = str.maketrans('','',string.punctuation)\n",
    "    texts = texts.str.replace('[{}]'.format(string.punctuation), '')\n",
    "    return texts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deal with input Twitter texts:<br>\n",
    "Use a for loop to travel all rows of our cleaning dataframe and<br>\n",
    "1. Truncate it(if the word can be found in feature map then set it to be idx of fhat feature, or set it to be MAX_WORD)\n",
    "2. Add padding if length of that rows of texts < MAX_SEQUENCE_LENGTH  <br>\n",
    "(make sure each texts after transform have same length)\n",
    "3. Output a nparay to be our model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sequence(texts):\n",
    "    texts = cleaning(texts)\n",
    "    \n",
    "    tokenized_texts = []\n",
    "    for string in texts:\n",
    "        tokenized_text = []\n",
    "        for idx, word in enumerate(string.split(' ')):\n",
    "            \n",
    "            # truncate\n",
    "            if idx >= MAX_SEQUENCE_LENGTH: break \n",
    "             \n",
    "            try:\n",
    "                token = token_dict[word.lower()]\n",
    "            except:\n",
    "                token = MAX_WORD\n",
    "            tokenized_text.append(token)\n",
    "            \n",
    "        # padding \n",
    "        if len(tokenized_text) < MAX_SEQUENCE_LENGTH: \n",
    "            tokenized_text.extend([MAX_WORD]*(MAX_SEQUENCE_LENGTH-len(tokenized_text)))\n",
    "                \n",
    "        tokenized_texts.append(tokenized_text)\n",
    "    return np.array(tokenized_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encode emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary to record different emotions types\n",
    "emotion_array = ['joy', 'sadness', 'trust', 'anticipation', 'fear', 'anger', 'disgust', 'surprise']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode emotion from text to idx<br>\n",
    "to_categorical -> change to matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode emotion from \"text\" to \"number\"(construct a dict to record)\n",
    "def emotion_to_sequence(emotions):\n",
    "    tokenized_emotions = []\n",
    "    for string in emotions:\n",
    "        token = emotion_array.index(string)\n",
    "        tokenized_emotions.append(token)\n",
    "    return to_categorical(tokenized_emotions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use functions above to deal with cleaned text datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequence = text_to_sequence(trains['cleaned']) # throw to model\n",
    "train_label = emotion_to_sequence(trains['emotion']) # answer for training data\n",
    "my_testing = text_to_sequence(trains['text']) # data for calculating f1-score\n",
    "text_sequence = text_to_sequence(tests['text']) # throw to model for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 256,512,512,batch512 -> ä¸éŒ¯ä¸éŽè¦åŠ å¤§epoch -> score: 0.39\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datetime import datetime \n",
    "\n",
    "# layer units information\n",
    "LSTM1_DIM = 256\n",
    "LSTM2_DIM = 128\n",
    "\n",
    "DENSE1_DIM = 512\n",
    "DENSE2_DIM = 512\n",
    "DENSE3_DIM = 512\n",
    "\n",
    "CATEGORY_NUM = 8 # we have 8 different emotions \n",
    "BATCH_SIZE = 512\n",
    "VALIDATION_SPLIT = 0.2 # 80% for training, 20% for validation\n",
    "EPOCHS = 5\n",
    "\n",
    "# logger to save information\n",
    "csv_logger = CSVLogger('training_log.csv')\n",
    "                    \n",
    "\n",
    "def model_setting(texts, labels, matrix_emb=\"uniform\"):\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    DROPOUT = 0.3 # prevent overfitting\n",
    "    \n",
    "    if matrix_emb != \"uniform\": matrix_emb = Constant(matrix_emb)\n",
    "        \n",
    "    # split data to train(0.8) & validation(0.2)\n",
    "    validation_size = int(texts.shape[0]*VALIDATION_SPLIT)\n",
    "    texts_train = texts[:-validation_size]\n",
    "    texts_val = texts[-validation_size:]\n",
    "    labels_train = labels[:-validation_size]\n",
    "    labels_val = labels[-validation_size:]\n",
    "    \n",
    "    # model build\n",
    "    model = Sequential()\n",
    "    # embedding layer -> deal with Word2vec input\n",
    "    model.add(Embedding(MAX_WORD+1, EMBEDDING_DIM, embeddings_initializer=matrix_emb, input_length=MAX_SEQUENCE_LENGTH, trainable=True)) # with embedding matrix\n",
    "    # LSTM layer (add dropout to prevent from overfitting)\n",
    "    model.add(LSTM(LSTM1_DIM, dropout=DROPOUT, recurrent_dropout=DROPOUT))\n",
    "    # dense layer1\n",
    "    model.add(Dense(DENSE1_DIM, kernel_initializer='normal', activation='relu'))\n",
    "    # dropout layer\n",
    "    model.add(Dropout(DROPOUT))\n",
    "    # dense layer2\n",
    "    model.add(Dense(DENSE2_DIM, kernel_initializer='normal', activation='relu'))\n",
    "    # dense layer3\n",
    "    model.add(Dense(DENSE3_DIM, kernel_initializer='normal', activation='relu'))\n",
    "    \n",
    "    # output layer\n",
    "    model.add(Dense(CATEGORY_NUM, kernel_initializer='normal', activation='softmax'))\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "    model.summary()\n",
    "\n",
    "    \n",
    "    history_callback = model.fit(\n",
    "        texts_train, \n",
    "        labels_train, \n",
    "        epochs=EPOCHS, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        callbacks=[csv_logger],\n",
    "        verbose=1, \n",
    "        shuffle=True, \n",
    "        validation_data=(texts_val, labels_val))\n",
    "\n",
    "    return history_callback, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_callback, mymodel = model_setting(train_sequence, train_label, word2vec_matrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate F1-score to check if our model trains well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### f1-score\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred_result = mymodel.predict(my_testing, batch_size=1024) # prediction\n",
    "y_pred_result = np.argmax(y_pred_result, axis=1) # decode\n",
    "results = [emotion_array[i]  for i in y_pred_result] # change from number to emotion\n",
    "print(classification_report(y_true=trains['emotion'], y_pred=results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict test data and save to .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predict\n",
    "pred_result = mymodel.predict(text_sequence, batch_size=1024)\n",
    "\n",
    "# Returns the indices of the maximum values along an axis\n",
    "pred_result = np.argmax(pred_result, axis=1) # use argmax to decode\n",
    "\n",
    "# change from number to emotion (using emotion_array)\n",
    "results = [emotion_array[i]  for i in pred_result]\n",
    "pred = tests\n",
    "pred['emotion'] = results\n",
    "# delete other information from dataframe\n",
    "pred = pred.drop('hashtags', axis=1)\n",
    "pred = pred.drop('text', axis=1)\n",
    "pred = pred.drop('identification', axis=1)\n",
    "pred.rename(columns={'tweet_id':'id'}, inplace=True) # format\n",
    "pred.head()\n",
    "pred.to_csv('prediction_{}.csv'.format(datetime.now()), index=False) # save file\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
